c:\Users\omare\Desktop\mlx projects\audio_everything\training.py:52: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.
  data = pd.read_csv(data_path, sep='\t')
Epoch 1/10:   0%|                                                                                                         | 0/156 [00:00<?, ?batch/s]c:\Users\omare\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\tokenization_utils_base.py:2852: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Epoch 1/10: 157batch [05:23,  2.06s/batch, loss=5.62]                                                                                                
Epoch 1 Loss: 7.00518855987451
Epoch 2/10: 157batch [05:15,  2.01s/batch, loss=5.15]                                                                                                                        
Epoch 2 Loss: 5.748240999686412
Epoch 3/10: 157batch [05:24,  2.07s/batch, loss=4.87]                                                                                                                        
Epoch 3 Loss: 5.504104638711
Epoch 4/10: 157batch [05:15,  2.01s/batch, loss=4.59]                                                                                                                        
Epoch 4 Loss: 5.3096126134579
Epoch 5/10: 157batch [05:23,  2.06s/batch, loss=4.35]                                                                                                                        
Epoch 5 Loss: 5.135786897096878
Epoch 6/10: 157batch [05:23,  2.06s/batch, loss=4.12]                                                                                                                        
Epoch 6 Loss: 4.9890494866248885
Epoch 7/10: 157batch [05:24,  2.07s/batch, loss=3.96]                                                                                                                        
Epoch 7 Loss: 4.8665764683332196
Epoch 8/10: 157batch [05:24,  2.07s/batch, loss=3.8]                                                                                                                         
Epoch 8 Loss: 4.755836760386442
Epoch 9/10: 157batch [05:24,  2.06s/batch, loss=3.67]                                                                                                                        
Epoch 9 Loss: 4.653993273392702
Epoch 10/10: 157batch [05:24,  2.07s/batch, loss=3.55]                                                                                                                       
Epoch 10 Loss: 4.564164971693968
